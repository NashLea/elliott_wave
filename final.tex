\documentclass[twocolumn,10pt]{asme2ej}
% \usepackage{fullpage,enumitem,amsmath,amssymb,graphicx,bm,listings,algpseudocode,hyperref,url,multirow,caption}
\usepackage{enumitem,amsmath,amssymb,graphicx,bm,listings,algpseudocode,hyperref,url,multirow,caption,graphicx}
\newcommand{\vect}[1]{\boldsymbol{#1}}

\lstdefinestyle{custom}{
  basicstyle=\footnotesize\ttfamily,
  language=Python,
}
\graphicspath{ {final/} }
\begin{document}

\title{Stock forecast system with Elliott Wave pattern recognition and adaptive trading strategy}
\author{Motonari ITO
  \affiliation{
    SUNet ID: motonari
    }
}

\maketitle

\begin{abstract}
  In this project, we have built a system to advise an optimal stock market
  trading policy; when and how many stocks you should buy or sell. We ran the
  system over various stock symbols and evaluated the result.

  The system consists of two components; reflex model based stock price
  predictors and a state model based trading policy evaluator and optimizer. The
  modularity allowed us to evaluate and optimize the performance of each
  component independently.
  
  The result indicates there is probably a weak correlation between the
  historical stock price behavior and the future stock prices. However, it was
  hard to build a sensible policy optimizer with the given set of data and
  training time.
\end{abstract}

\section{Introduction}

Stock forecast has been studied and practiced with various degree of
success. The technical analysis is a methodology based on the
historical stock market prices \cite{wiki:technical_analysis}. The
fundamental analysis tries to predict based on the business's
financial statement \cite{wiki:fundamental_analysis}. Data mining over
the Internet with sentiment analysis also became popular recently
\cite{web:data_mining_analysis}.

Elliott Wave Principle (EWP) is a classical technical analysis method
\cite{frost1981elliott, web:study_of_cycles}. It is a hypothesis that stock
market price can be modeled as a sequence of waves which shapes follow some
defined rules. EWP suggests we can predict the future market price more
accurately than a random chance by recognizing the wave pattern.  This is
distinct from other stock price prediction methods in that it relies solely on
the historical price changes and doesn't use external information such as market
sentiment or industrial news.

EWP has been criticized for the poor performance
\cite{aronson2007evidence-based}. Notably, for a given stock
historical data, the rules yields many different interpretation of the
wave shapes. This uncertainty makes the future prediction hard, if not
impossible, while one can claim the accuracy of the theory
\emph{after} the fact. In a sense, EWP is so powerful and complex
model that it easily falls into over-fitting.

We, however, believe the essence of EWP is valid; future price is influenced by
past price pattern. Intuitively, some often predict the price to go up if the
past price has strong upward trend. Others may predict downward trend if the
past price shows inverted-V shape. While such prediction may not be an inherent
property of the stock market, the fact that many people believe that way affect
the market. Therefore, it is probable that a reflex based machine learning
algorithm can predict a future price based on the past prices.

There is also several attempt to predict the stock price based on the sentiment
analysis over various data source on the Internet \cite{arXiv:1010.3003}. In the
project, we tried to use New York Times Community API
\cite{web:nytimes_community_api} to retrieve the customer comments and use them
as a hint to a predictor.

Given we have a sensible predictor, it is still an open question when and how to
trade stocks to optimize the asset because the predictor is inherently
imperfect. For example, it may not be smart to sell the entire stocks asset
immediately just because a predictor says price might go down. The trading
decision should be educated by the actual performance of predictors.

Our approach is to use a state based learning algorithm to find the
optimal trading strategy. Intuitively, as it runs the predictors on
the historical data, the trader will learn the peculiarity of each
predictor. 

\section{System}

The system consists of two parts: predictors and traders.

A predictor predicts a future price change of a particular
stock. Each predictor uses different input data and inference
algorithm. We evaluate the performance of each implementation and
plug-in some of the best predictor to the final system.

A trader uses the prediction and learns the optimal trading policy:
when to buy/sell how much stocks.

There are five predictors (SimpleNNPredictor, LinearPredictor,
SentimentPredictor, PatternPredictor, and CheatPredictor) and one
trader (QTrader). 

For the purpose of the discussion, we use the following definitions. More
variables will be defined as needed.

\begin{align*}
p_i &:= \text{Stock price of the }i\text{-th day} \\
priceChange(p_{old}, p_{new}) &:= \frac{p_{new} - p_{old}}{p_{old}}
\end{align*}

\subsection{Predictors}

All the predictors are configured with a hyper parameter $D$, which indicates
the future date delta to predict. 

They also implement three operations.

\begin{description}
  \item[extractFeatures] For a given date, the function returns the feature
    vector for the prediction.
  \item[train] The function is invoked with a feature vector and the target
    value, which is the price change after $D$ days.
  \item[predict] The function is invoked with a feature vector and returns the
    predicted price change after $D$ days.
\end{description}

\subsubsection{SimpleNNPredictor}
The predictor uses the multilayer perception implementation from
scikit-learn \cite{web:scikit_learn}. It extract the feature by
looking back the prior stock price.

The look back date is defined as a vector $\vect{b}$:

\[
\vect{b} \gets [89, 55, 34, 21, 13, 8, 5, 3, 2, 1]
\]

In the training phase, it looks back the stock prices and calculate
the price changes $X$ compared to the current price. Suppose the
current date index is denoted as $i$, the input to the algorithm is:

\[
X_i \gets \left\{priceChange(p_{i - j}, p_i) : j \in \vect{b}\right\}
\]

The target value is is the actual stock price change for the given
future date: $priceChange(p_i, p_{i+D})$.

The algorithm uses two hidden layers; 3 and 2 nodes each.

\subsubsection{LinearPredictor}
The predictor is same as SimpleNNPredictor except that the underlying
algorithm uses a linear regression with stochastic gradient descent,
also from scikit-learn \cite{web:scikit_learn}.

\subsubsection{SentimentPredictor}

The predictor uses New York Times Community API
\cite{web:nytimes_community_api} to retrieve the customer comments of
each news article from Jan 1, 2010 to Nov 10, 2016.

Then, it uses Stanford Core NLP \cite{manning-EtAl:2014:P14-5} to find
the sentiment. The algorithm returns a tuple of sentimentValue and
sentiment for each sentence in the comment. For example, a very
positive sentence may return (3, 'positive'). A weakly negative
sentence would return (1, 'negative').

We estimate the sentiment of the comment by averaging the sentiment
values.

\begin{verbatim}
def comment_sentiment(comment):
  total = 0
  score = 0
  for each sentence in comment:
    if sentiment == 'positive':
      score += sentimentValue
      total += sentimentValue
    elif sentiment == 'negative':
      score -= sentimentValue
      total += sentimentValue
    else:
      total += sentimentValue

  return float(score) / total
\end{verbatim}

We take a comment which contains a related word to a stock symbol. For
example, for \verb|aapl|, we pick a comment with words 'aapl',
'apple', 'iphone', 'ipad', 'mac', 'ipod', or 'ios'.

Then, for a given date, we look back the last 30 days of the sentiment
for the stock and create a feature vector of size 30.

We use a linear regression with stochastic gradient descent from
scikit-learn \cite{web:scikit_learn} to train the predictor. The
target value is is the actual stock price change for the given future
date: $priceChange(p_i, p_{i+D})$.

\subsubsection{PatternPredictor}

The predictor uses a table look up method.

It extracts the feature vector $\phi(x)$ by looking back the prior
stock price. The look back date is defined as a vector $\vect{b}$:

\[
\vect{b} \gets [0, 1, 2, 3, 5, 8, 13, 21, 34]
\]

It looks back the stock prices and map the price changes of each
interval to $+1$ (= price went up) or $-1$ (= price went down). When
there is no price change, we pick $+1$ or $-1$ randomly.

For example, suppose we have the stock price history in table
\ref{patternStockExample}, the the feature vector would be
$\phi(x)=[-1,-1,-1,+1,+1,0,+1,+1]$.

\begin{table}
  \begin{tabular}{cc}
    n-days ago & price (dollar) \\
    \hline
    today & 100 \\
    1 & 98 \\
    2 & 95 \\
    3 & 95 \\
    5 & 93 \\
    8 & 92 \\
    13 & 96 \\
    21 & 99 \\
    34 & 100 \\
  \end{tabular}
  \caption{Example stock price change}
  \label{patternStockExample}
\end{table}

The feature vector is used as a key of the table look up. Since there
are eight intervals where each interval gets one of two values, there
will be $256$ entries in the table.

For the target value, the future price $p_{i+D}$ is mapped to $+1$,
$-1$, or $0$ compared to the current price $p_i$. Note that we use $0$
when there is no price change.

In the training phase, the table is updated to the average of the
target value $y$ as follows.

\begin{align*}
  \eta &\gets \frac{1}{\text{\# of updates to the entry}} \\
  \text{table}[\phi(x)] &\gets (1 - \eta)\text{table}[\phi(x)] + \eta y \\
\end{align*}

In the prediction phase, we simply look up the table and return the
estimated target value.

\subsubsection{CheatPredictor}

The predictor returns the future price changes by actually looking at the
data. It is used for the testing purpose.

\subsection{Trader}

If we had a perfect predictor, the optimal strategy would have been to
buy before the stock price goes up and to sell before it goes
down. However, no predictor is perfect.

To learn the optimal trading policy given the uncertain predictions,
we use a reinforcement learning.

For the sake of discussion, we define some additional variables here.
\begin{align*}
  o_i &:= \text{The number of stocks owned on }i\text{-th day.} \\
  c_i &:= \text{the maximum number of stocks}\\
  & \text{we could buy with our current cash on }i\text{-th day.} \\
  m_i &:= \text{the predicted slope by performing a least square} \\
  & \text{polynomial fit over the predicted future price changes.} \\
  r_i &:= \text{the sum of residuals of the predicted slope above.} \\
\end{align*}
The goal is to find a trading policy to maximize our asset value $u$
after the sequence of trades. Suppose the last day index is $n$, the
asset value $u_n$ is defined:

\[
u_n = p_n(o_n + c_n)
\]

\subsubsection{Model}

We model the problem as MDP where we don't know the transition function.

\begin{description}
\item[State] For $i$-th day, the state is defined to have a tuple $(o_i,c_i,m_i,r_i)$.

\item[Initial State] Before running MDP starting at day index $start$,
  we initialize the state as follows.
  \begin{itemize}
  \item $o_{start} = 0$
  \item $o_{start} = 10$
  \item $m_{start}$ and $r_{start}$ are initialized by a predictor
    based on the first day.
  \end{itemize}
  
\item[Action] The action is an integer in the range: $[-o_i,c_i]$. The
  negative value means to sell owned stocks for that amount, and the
  positive value means to buy stocks for that amount.

\item[Transition] On taking a action, the state moves to the next
  day. Note that the system doesn't know the next state beyond today.

\item[Reward] The reward is the difference of asset value before and
  after the state transition. 
  
  \[
  Reward = p_i(o_{i} + c_{i}) - p_{i-1}(o_{i-1} + c_{i-1})
  \]
  
  Intuitively, we want to have more stocks when the stock price is
  high and we want to have more cash when the stock price is low.

\item[EndState] $o_i < 1$ or the current date hits end of the period.
\end{description}

\subsubsection{Algorithm}

We use Q-learning with a function approximation and epsilon-greedy
learning.

\subsubsection*{Parameters}

\begin{align*}
  \epsilon &:= \text{Parameter for epsilon-greedy policy} \\
  \phi(state,action) &:= \text{feature extractor} \\
  \vect{w} &:= \text{weight to learn} \\
  \hat{Q}_{opt}(s, a; \vect{w}) &:= \vect{w} \cdot \phi(s, a) \\
\end{align*}


\subsubsection*{Choose an action}

We examine the state and obtain the available actions: $[-o_k,c_k]$.

Based on whether a random number $[0.0, 1.0]$ is greater than
$\epsilon$, we pick exploration or exploitation.

\[
  \pi_{act}(s) = \\
  \begin{cases}
    \arg \max_{a \in actions}\hat{Q}_{opt}(s, a) & \text{probability } 1-\epsilon \\
    \text{random choice from actions} & \text{probability } \epsilon \\
  \end{cases}
\]


\subsubsection*{Calculate reward}

Based on the action, we calculate the reward.

\[
  reward = p_{i+1}(o_{i+1} + c_{i+1}) - p_{i}(o_{i} + c_{i})
\]

\subsubsection*{Transition to the next state}

When an action is taken, MDP moves to the next state, which represents
the stock market and the current asset of the next day.

\begin{align*}
  c_{i+1} &\gets (c_{i} - action)\frac{p_i}{p_{i+1}} \\
  o_{i+1} &\gets o_{i} + action \\
  m_{i+1}, r_{i+1} &\gets \text{predict(i+1)} \\
\end{align*}

\subsubsection*{Feature Extraction}

We have two different feature extractors for the function approximation; simple
and complex.

The simple feature extractor uses just one feature; a product of the predicted
future price slope and the action value. We use this just for testing purpose.

\[
\phi_s(s_i,a_{i+1}) := [m_i \times a_{i+1}]
\]

The complex feature extractor uses some combinations of the simple feature. We
use this to predict the actual stock behavior.

\[
\phi_c(s_i,a_{i+1}) := [m_i \times a_{i+1}, m_i^2 \times a_{i+1}, m_i \times a_{i+1}^2, m_i^2 \times a_{i+1}^2 ]
\]

\subsubsection*{Update weights}

We update the weights as follows.
\begin{align*}
\hat{V}_{opt}(s') &\gets \max_{a \in actions(s')}\hat{Q}_{opt}(s', a;\vect{w}) \\
\vect{w} &\gets \vect{w} - \eta[\hat{Q}_{opt}(s, a;\vect{w}) - (reward + \gamma \hat{V}_{opt}(s'))\phi(s,a)
\end{align*}

\subsubsection{Test}

In test phase, we run the learning algorithm on a new data with the
following modification.

\begin{itemize}
\item Always choose the optimal action, that is to set $\epsilon = 0$
\item Skip weight update step.
\end{itemize}

Then, see if how much money earned or lost by looking at the final
asset value.


\section{Performance}

\subsection{Predictors}

We use stock price data from Yahoo Finance
\cite{web:yahoo_finance}. It provides day-to-day closing stock price
for the period shown in table \ref{yahooStockData}.

\begin{table}
  \begin{tabular}{ccc}
    Symbol & Start Date & End Date \\
    \hline
    aapl & 1980-12-12 & 2016-11-11 \\
    bp & 1977-01-03 & 2016-11-11 \\
    cop & 1981-12-31 & 2016-11-11 \\
    cost & 1986-07-09 & 2016-11-11 \\
    cvx & 1970-01-02 & 2016-11-11 \\
    dj & 1985-01-29 & 2016-11-11 \\
    hd & 2011-11-14 & 2016-11-11 \\
    ibm & 1962-01-02 & 2016-11-11 \\
    ko & 1962-01-02 & 2016-11-11 \\
    low & 2011-11-14 & 2016-11-11 \\
    nke & 1980-12-02 & 2016-11-11 \\
    qcom & 1991-12-13 & 2016-11-11 \\
    rut & 1987-09-10 & 2016-11-11 \\
    tgt & 1980-03-17 & 2016-11-11 \\
    wmt & 2011-11-14 & 2016-11-11 \\
    xcom & 1970-01-02 & 2016-11-11 \\
  \end{tabular}
  \caption{Yahoo Finance Stock Data}
  \label{yahooStockData}
\end{table}


We measure the performance of predictors by whether it predicts
upward/downward trend correctly on the test data. For example, if the
predictor predicts upward trend correctly, we call it as true
positive. 

Each predictor is trained over all the stock symbols up to
2015-11-11. The rest of the one year data is used for testing.

In one training iteration, for each stock symbol, we pick a date
randomly and run through the predictor for the next 128 days. Since we
have 16 stock symbols, one iteration feeds $128 * 16 = 2048$ training
samples (note: they are not necessarily unique) to the predictor.


\subsubsection{SimpleNNPredictor}

Table \ref{pred-perf-SimpleNNPredictor-1-398}, \ref{pred-perf-SimpleNNPredictor-3-398}, and
\ref{pred-perf-SimpleNNPredictor-7-398} shows prediction performance of
SimpleNNPredictor after 398 iteration. We can observe the accuracy is slightly better than 50\%.

\begin{table}
  \begin{tabular}{cc}
    \begin{tabular}{cc|cc}
      & & \multicolumn{2}{c}{Predicted} \\
      & & $+ $ & $-$ \\
      \hline
      \multirow{2}{*}{Actual}
      & $+$ & 0.15 & 0.20 \\
      & $-$ & 0.28 & 0.37 \\
      \hline
    \end{tabular}
    &
    \begin{tabular}{cc}
      Accuracy & 0.52 \\
      F1 Score & 0.38 \\
    \end{tabular}
  \end{tabular}
  \caption{SimpleNNPredictor, $D=1$, 398 iterations}
  \label{pred-perf-SimpleNNPredictor-1-398}
\end{table}


\begin{table}
  \begin{tabular}{cc}
    \begin{tabular}{cc|cc}
      & & \multicolumn{2}{c}{Predicted} \\
      & & $+ $ & $-$ \\
      \hline
      \multirow{2}{*}{Actual}
      & $+$ & 0.40 & 0.10 \\
      & $-$ & 0.39 & 0.11 \\
      \hline
    \end{tabular}
    &
    \begin{tabular}{cc}
      Accuracy & 0.51 \\
      F1 Score & 0.62 \\
    \end{tabular}
  \end{tabular}
  \caption{SimpleNNPredictor, $D=3$, 398 iterations}
  \label{pred-perf-SimpleNNPredictor-3-398}
\end{table}


\begin{table}
  \begin{tabular}{cc}
    \begin{tabular}{cc|cc}
      & & \multicolumn{2}{c}{Predicted} \\
      & & $+ $ & $-$ \\
      \hline
      \multirow{2}{*}{Actual}
      & $+$ & 0.28 & 0.23 \\
      & $-$ & 0.24 & 0.24 \\
      \hline
    \end{tabular}
    &
    \begin{tabular}{cc}
      Accuracy & 0.52 \\
      F1 Score & 0.54 \\
    \end{tabular}
  \end{tabular}
  \caption{SimpleNNPredictor, $D=7$, 398 iterations}
  \label{pred-perf-SimpleNNPredictor-7-398}
\end{table}


However, figure \ref{pred-perf-SimpleNNPredictor-7-f1} shows that the predictor
performance, in terms of F1 score, does not improve over the training
iteration. 

\begin{figure}
  \centering
  \includegraphics[width=5cm]{pred-perf-SimpleNNPredictor-7-f1}
  \caption{SimpleNNPredictor F1 score by the number of iterations}
  \label{pred-perf-SimpleNNPredictor-7-f1}
\end{figure}

\subsubsection{LinearPredictor}

Table \ref{pred-perf-LinearPredictor-1-398},
\ref{pred-perf-LinearPredictor-3-398}, and
\ref{pred-perf-LinearPredictor-7-398} shows prediction performance of
LinearPredictor after 398 iteration. Although we can observe the
accuracy is slightly better than 50\%, the confusion matrix indicates
strong bias in the prediction.

\begin{table}
  \begin{tabular}{cc}
    \begin{tabular}{cc|cc}
      & & \multicolumn{2}{c}{Predicted} \\
      & & $+ $ & $-$ \\
      \hline
      \multirow{2}{*}{Actual}
      & $+$ & 0.00 & 0.35 \\
      & $-$ & 0.00 & 0.65 \\
      \hline
    \end{tabular}
    &
    \begin{tabular}{cc}
      Accuracy & 0.65 \\
      F1 Score & 0.00 \\
    \end{tabular}
  \end{tabular}
  \caption{LinearPredictor, $D=1$, 398 iterations}
  \label{pred-perf-LinearPredictor-1-398}
\end{table}


\begin{table}
  \begin{tabular}{cc}
    \begin{tabular}{cc|cc}
      & & \multicolumn{2}{c}{Predicted} \\
      & & $+ $ & $-$ \\
      \hline
      \multirow{2}{*}{Actual}
      & $+$ & 0.50 & 0.00 \\
      & $-$ & 0.50 & 0.00 \\
      \hline
    \end{tabular}
    &
    \begin{tabular}{cc}
      Accuracy & 0.50 \\
      F1 Score & 0.67 \\
    \end{tabular}
  \end{tabular}
  \caption{LinearPredictor, $D=3$, 398 iterations}
  \label{pred-perf-LinearPredictor-3-398}
\end{table}


\begin{table}
  \begin{tabular}{cc}
    \begin{tabular}{cc|cc}
      & & \multicolumn{2}{c}{Predicted} \\
      & & $+ $ & $-$ \\
      \hline
      \multirow{2}{*}{Actual}
      & $+$ & 0.51 & 0.00 \\
      & $-$ & 0.49 & 0.00 \\
      \hline
    \end{tabular}
    &
    \begin{tabular}{cc}
      Accuracy & 0.51 \\
      F1 Score & 0.68 \\
    \end{tabular}
  \end{tabular}
  \caption{LinearPredictor, $D=7$, 398 iterations}
  \label{pred-perf-LinearPredictor-7-398}
\end{table}

Figure \ref{pred-perf-LinearPredictor-7-f1} shows that the predictor
performance, in terms of F1 score, does not improve over the training
iteration. 

\begin{figure}
  \centering
  \includegraphics[width=5cm]{pred-perf-LinearPredictor-7-f1}
  \caption{LinearPredictor F1 score by the number of iterations}
  \label{pred-perf-LinearPredictor-7-f1}
\end{figure}

\subsubsection{PatternPredictor}

Table \ref{pred-perf-PatternPredictor-1-398},
\ref{pred-perf-PatternPredictor-3-398}, and
\ref{pred-perf-PatternPredictor-7-398} shows prediction performance of
PatternPredictor after 398 iteration.

This predictor shows sensible performance. The accuracy is almost
always slightly more than 50\%. 

\begin{table}
  \begin{tabular}{cc}
    \begin{tabular}{cc|cc}
      & & \multicolumn{2}{c}{Predicted} \\
      & & $+ $ & $-$ \\
      \hline
      \multirow{2}{*}{Actual}
      & $+$ & 0.21 & 0.14 \\
      & $-$ & 0.36 & 0.28 \\
      \hline
    \end{tabular}
    &
    \begin{tabular}{cc}
      Accuracy & 0.50 \\
      F1 Score & 0.46 \\
    \end{tabular}
  \end{tabular}
  \caption{PatternPredictor, $D=1$, 398 iterations}
  \label{pred-perf-PatternPredictor-1-398}
\end{table}


\begin{table}
  \begin{tabular}{cc}
    \begin{tabular}{cc|cc}
      & & \multicolumn{2}{c}{Predicted} \\
      & & $+ $ & $-$ \\
      \hline
      \multirow{2}{*}{Actual}
      & $+$ & 0.34 & 0.16 \\
      & $-$ & 0.32 & 0.18 \\
      \hline
    \end{tabular}
    &
    \begin{tabular}{cc}
      Accuracy & 0.52 \\
      F1 Score & 0.58 \\
    \end{tabular}
  \end{tabular}
  \caption{PatternPredictor, $D=3$, 398 iterations}
  \label{pred-perf-PatternPredictor-3-398}
\end{table}


\begin{table}
  \begin{tabular}{cc}
    \begin{tabular}{cc|cc}
      & & \multicolumn{2}{c}{Predicted} \\
      & & $+ $ & $-$ \\
      \hline
      \multirow{2}{*}{Actual}
      & $+$ & 0.38 & 0.13 \\
      & $-$ & 0.34 & 0.15 \\
      \hline
    \end{tabular}
    &
    \begin{tabular}{cc}
      Accuracy & 0.53 \\
      F1 Score & 0.62 \\
    \end{tabular}
  \end{tabular}
  \caption{PatternPredictor, $D=7$, 398 iterations}
  \label{pred-perf-PatternPredictor-7-398}
\end{table}

Figure \ref{pred-perf-PatternPredictor-7-f1} shows that the predictor
performance, in terms of F1 score, improves as we increase the
training iteration.

\begin{figure}
  \centering
  \includegraphics[width=5cm]{pred-perf-PatternPredictor-7-f1}
  \caption{PatternPredictor F1 score by the number of iterations}
  \label{pred-perf-PatternPredictor-7-f1}
\end{figure}

\subsubsection{SentimentPredictor}

\subsection{Trader}

We measured the performance of the trader by comparing the asset value between
the beginning and the end of the episode.

First, we split the data set to training set (till 2015-11-11) and development
set (after 2015-11-11). 

Then, we trained the predictors on the training set. Based on the predictor
performance analysis in figure \ref{pred-perf-PatternPredictor-7-f1}, we use 100
iteration for the predictor training. We use three predictors of a same kind;
$D=1$, $D=3$, and $D=7$.

With the trained predictors, we run \verb|QTrader| on the training set. In the
training, we pick a start date index randomly and run an episode up to 90
days. The process is repeated for the specified number of times:
traderTrainingLoopCount.

\subsection{CheatPredictor}

Before trying a complex scenario, we need to verify the correctness of the
trader implementation. we use \verb|CheatPredictor| and the simpified feature
extractor $\phi_s(s,a)$ so that the performance is not impacted by the predictor
and we can reason the resultant weights easily.

In this simplified scenario, even with one iteration, the trader quickly learned
the weight value of $[0.0873]$. Recall that $\phi_s(s,a)$ is defined as $[m_i
  \times a_{i+1}]$. The positive weight value means:

\begin{itemize}
  \item When the predicted future price is upward ($m_i > 0$), the optimal
    action to maximize $\hat{Q}_{opt}(s, a)$ is the max value of action; buy as
    many stocks as possible.

  \item When the predicted future price is downward ($m_i < 0$), the optimal
    action to maximize $\hat{Q}_{opt}(s, a)$ is the min value of action; sell as
    many stocks as possible.
\end{itemize}

Therefore, the trader implementation appears to be sensible.

\subsection{PatternPredictor}

According to the predictor analysis, \verb|PatternPredictor| appears to be the
best. We have conducted two test cases; the first case is to use one instance of
\verb|PatternPredictor| with $D=1$ and the other case is to use three instances
with $D=[1, 3, 7]$.

Table \ref{trader-perf-PatternPredictor-1000} shows the result. Unfortunately,
we are not convinced that the system made more money than a chance result.

\begin{table}
  \begin{tabular}{crr}
    & \multicolumn{1}{c}{Gain (US\$)} \\
    Symbol & $D=1$ & $D=[1,3,7]$ \\
    \hline
    dj &  13945.71  & -3142.03 \\
    qcom &  -28.77  & -2.29 \\
    rut &  1388.12  & 175.32 \\
    wmt &   133.03  & 6.54 \\
    hd &    -59.87  & -34.99 \\
    low &   -52.70  & 4.87 \\
    tgt &    60.83  & 32.07 \\
    cost & -264.94  & -136.91\\
    nke &    84.20  & -16.31 \\
    ko &    -25.74  & -8.51 \\
    xom &    30.16  & -10.20 \\
    cvx &   312.85  & 73.53 \\
    cop &   -32.72  & 61.35 \\
    bp &     20.58  & -2.43 \\
    ibm &    28.37  & 37.15 \\
    aapl &    2.57  & 43.06 \\
  \end{tabular}
  \caption{Trader with PatternPredictor, 1000 iterations}
  \label{trader-perf-PatternPredictor-1000}
\end{table}

\section{Conclusion}

We have successfully confirmed, with the result of \verb|PatternPredictor|, the
stock market behavior can be predicted by the prior price changes.

The predictor uses table look up algorithm as opposed to any generalization
algorithm such as a linear regression and a neural network. Other predictors
(\verb|SimpleNNPredictor| and \verb|LinearPredictor|) didn't perform well. 

It is, in a sense, supported by Elliott Wave Theory, which focuses on the wave
shape instead of the exact prices. However, by simplifying the stock market
behavior into a table of 256 entries, we are afraid we miss some important
features. Ultimately, \verb|PatternPredictor| performs just silightly better
than 50\% accuracy. We might be able to design better predictor by using more
complex model (more hidden layers and/or features) -- it's a future research
work.

Another predictor, \verb|SentimentPredictor|, may have potential but didn't
perform well in the current configuration. We suspect it's due to the lack of
data. Unfortunately, we couldn't find any convenient free data source other than
New York Times API (For example, Twitter API doesn't allow us to access older
tweets than a few weeks anymore).

The trader had even harder challenges. 




\bibliographystyle{plain}
\bibliography{final,StanfordCoreNlp2014}

\end{document}



