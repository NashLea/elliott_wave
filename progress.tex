\documentclass[12pt]{article}
\usepackage{fullpage,enumitem,amsmath,amssymb,graphicx,bm,listings,algpseudocode,hyperref}
\newcommand{\vect}[1]{\boldsymbol{#1}}

\lstdefinestyle{custom}{
  basicstyle=\footnotesize\ttfamily,
  language=Python,
}
\begin{document}

\begin{center}
{\Large CS221 Fall 2016 Project [p-progress]}

\begin{tabular}{rl}
SUNet ID: & motonari \\
Name: & Motonari ITO \\
Collaborators: & Sundararaman Shiva
\end{tabular}
\end{center}

\section{Scope}

\subsection{Elliott Wave Principle}

Elliott Wave Principle (EWP) is a hypothesis that stock market price
can be modeled as a sequence of waves which shapes follow some defined
rules. EWP suggests we can predict the future market price more
accurately than a random chance by recognizing the wave pattern.

This is distinct from other stock price prediction methods in that it
relies sololy on the historical price changes and doesn't use external
information such as market sentiment or industrial news. While we
could improve the prediction by using those methods complementary, for
this project, we focus on EWP approach.

While EWP has several rules about wave shapes, we don't use them in
the original form (except in the baseline algorithm explored in
\verb|p-proposal|.) Instead, we use Reinforcement Learning to find a
new set of rules which predict the price more accurately.

\subsection{Input and Output}

The system learns stock price patterns from historic price data
obtained from Yahoo Finance
(\url{https://finance.yahoo.com/quote/AAPL/history?p=AAPL}) and
advises the optimal action (buy or sell) for today.

For example,
\begin{description}
\item[Input] Historic stock price data as an array and the currently
  own stocks as a dictionary: \{PurchaseDate, NumberOfStocks\}.
  
  \begin{itemize}
  \item AAPL stock data (Dec 12, 1980 - Oct 22, 2016)
  \item \{Nov 12 2015: 121 stocks, Apr 1 2016: 53 stocks, ...\}
  \end{itemize}
\item[Output]
  buy or sell suggestion for today. For example, \{Sell: \{ Nov 12 2015: 11 stocks, Apr 1 2016: 5 stocks \}, Buy: 12 stocks \}.
\end{description}

\section{Model}

We model a stock market as an MDP where we don't know the
transition function.

\subsection{State}

The state consists of prior stock price change and amount of currently owned stocks. 

\begin{itemize}
\item Let $p_i$ be the closing price $N[i]$ days ago, where $N \in [87,54,33,21,13,8,5,3,2,1]$.
  We use a set of Boolean indicating the price went up or down between each day, which is $\{I[p_i < p_{i+1}]\}$, as a
  state. ($I[..]$ is an indicator variable.)

  Note that we use Fibonacci numbers because EWP suggests there are
  some correlation between the stock market behavior and Fibonacci
  numbers.
  
\item Currently owned stocks as an array of tuple:
  \begin{itemize}
  \item Price difference between purchased date and today
  \item The number of owned stocks
  \end{itemize}
  
\end{itemize}

\subsection{Action}

On a given day, we perofrm a subset of these operations as an action. The subset can be an empty set, which represents
no trade for the day.

\begin{itemize}
\item Buy $N$ stock.
\item Sell stock(s). It specifies which stocks to sell in the
  currently owned stocks array. 
\end{itemize}

For example, action: \{Sell: \{ Nov 12 2015: 11 stocks, Apr 1 2016: 5 stocks \}, Buy: 12 stocks \} means to sell 11
stocks purchased on Nov 12 2015, sell 5 stocks purchased on Apr 1 2016, and buy 12 new stocks.

\subsection{Reward}

We get reward according to the number of sold stocks and the
current price. 

\subsection{Transition}

After the action, we move to the next day, which has a new state based
on the stock price and the prior action.

When the state reaches today, the system reports the action of the day
which would results in the maximum reward.

\section{Algorithm}

We use Q-learning with Epsilon-greedy and function approximation. We
use the following variables.

\begin{align*}
  \epsilon &:= \text{Parameter for epsilon-greedy policy} \\
  \phi(state,action) &:= \text{feature extractor} \\
  p_i &:= \text{stock price of i-th day} \\
  \vect{w} &:= \text{weight to learn} \\
  \hat{Q}_{opt}(s, a; \vect{w}) &:= \vect{w} \dot \phi(s, a) \\
\end{align*}

\subsection{Learning}

In learning phase, we loop through the stock price of each day.

\subsubsection*{Choose an action}

We examine the state and obtain the available actions. For example, while we can always buy a stock, we cannot sell a
stock if we don't own one.

Based on whether a random number $[0.0, 1.0]$ is greater than
$\epsilon$, we pick exploration or exploitation.

\[
  \pi_{act}(s) = \\
  \begin{cases}
    \arg \max_{a \in actions}\hat{Q}_{opt}(s, a) & \text{probability} 1-\epsilon \\
    \text{random from actions} & \text{probability} \epsilon \\
  \end{cases}
\]

\subsubsection*{Calculate reward}

Based on the action, we calculate the reward.

\[
  reward = \\
  \begin{cases}
    -p_i \times \text{number of stocks to buy} & \text{``buy'' action} \\
    p_i \times \text{number of stocks to sell} & \text{``sell'' action} \\
    0 & \text{no action} \\
  \end{cases}
\]

\subsubsection*{Find next state}

Let $s' := \text{state}$. Based on the action, we update $s'$.

\begin{itemize}
\item When we bought stocks, add \verb|(0, number of stocks)| to
  the currently own stock array. It means we have stocks with zero
  price difference.
  \item When we sold stocks, subtract the number of stocks.
\end{itemize}

Then, we update $s'$ for the next day. The historical prices are
shifted. The price differences in the currently owned stocks are
updated based on the new stock price.

\subsubsection*{Update weights}

We update the weights as follows.
\begin{align*}
&\hat{V}_{opt}(s') = \max_{a \in actions(s')}\hat{Q}_{opt}(s', a;\vect{w}) \\
&\vect{w} \leftarrow \vect{w} - \eta[\hat{Q}_{opt}(state, action;\vect{w}) - (reward + \gamma \hat{V}_{opt}(s'))\phi(state,action)
\end{align*}

\subsection{Test}

In test phase, we run the learning algorithm on a new data with the
following modification.

\begin{itemize}
\item Always choose the optimal action, that is to set $\epsilon = 0$
\item Skip weight update step.
\end{itemize}

Then, see if how much money earned or lost by looking at the sum of
all rewards.

\section{Preliminary Implementation}

In the current preliminary implementation, we have made the following
simplification.

\begin{itemize}
\item Feature extractor $\phi(s, s)$ returns
  \verb|list(priorPattern) + [len(currentAssets)] + [action]|, which
  contains only the total number of current assets and no
  generalization on the prior stock price change.

  \item Actions are limited to buy or sell only one stock per day.
\end{itemize}

We run the algorithm over various stock over one year up to today. We lost a lot of money. Also, in some stocks, the
algorithm thinks not buying or selling any stocks is the optimal choice. We hope to improve the algorithm by lifting up
the implementation limitation above.

\begin{tabular}{|c|c|}
  \hline 
dj & -1486.736328 \\ 
qcom & -61.536490 \\ 
rut & 268.510009 \\ 
wmt & 212.577556 \\ 
hd & 0.000000 \\ 
low & -12.438588 \\ 
tgt & 0.000000 \\ 
aapl & 4.597696 \\
\hline
\end{tabular}

\end{document}
